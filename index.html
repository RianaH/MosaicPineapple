<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">English Proficiency Detection and Generation</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team Mosaic Pineapple</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              Nhi Dang
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Riana Hoagland
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              Harrison Wallander
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Language proficiency assessment using the CEFR scale is traditionally accomplished by human language experts, but this task's subjective nature leads to inconsistencies in assessment results due to differing views of sentence complexity between experts. This has led to interest in research regarding the feasibility of using various machine learning methods and models for proficiency assessment. While other papers have focused on customized and/or fine-tuned methods for this task, and have found that certain algorithms and models can be optimized to be very accurate, we sought to find out how effective already existing open-source models like OpenAI's ChatGPT and Meta's Llama are at this task. We chose this approach because having commonly-available and consistent methods for automatic proficiency assessment could prove very useful in a number of fields including education, immigration, and work authorization. 

To this end, we devised an experiment in which we used 3 set prompts in conjunction with 10,000 CEFR-labeled English sentences to test the effectiveness of ChatGPT, Llama1B, Llama3B, and Llama8B at the task of language proficiency. Our main findings are that both ChatGPT and Llama1B approach 90\% accuracy in classifying sentences within 1 CEFR level of their true level, while Llama3B and Llama8B tend to lag behind. Additionally, we found that the better a model is at classifying overall, the more consistent it is at classifying more "subjective" sentences according to experts. This means that both ChatGPT and Llama1B could be effectively used for automatic proficiency detection tasks, and that their results could be trusted to be both somewhat accurate, and more importantly, consistent.</p>

<hr>

<h2 id="teaser">Overview </h2>

<img src="https://raw.githubusercontent.com/RianaH/MosaicPineapple/main/one.jpg" alt="Process Overview" width="600" height="400">

<hr>

<h2 id="introduction">Introduction </h2>

<h3 id="problemoverview">Problem Overview </h3>    
<p>
Language proficiency assessment is a critical task with a wide range of uses in education, immigration, and work authorization. One of the most common methods for classifying a person's proficiency in a language is the Common European Framework of Reference for Languages (CEFR) scale, which assigns a person's proficiency in a given language to one of 6 levels (A1, A2, B1, B2, C1, and C2) in increasing order of proficiency. These levels are defined by a number of different aspects of the language to be classified, including vocabulary, sentence structure, and grammar. To some extent, this scale is inherently dependent on the interpretation of the person who classifies each sample, which often leads to situations in which two people may assign the same sample of a language to different levels on the CEFR scale. This has lead to a desire in recent years for an automatic language proficiency assessment, which would be unbiased and able to fully standardize the act of classifying a person's proficiency in a given language.</p>
<p>

<h3 id="background">Background </h3> 
<p>
Significant research has already been done into the topic of using various machine learning techniques to classify text by proficiency level. The majority of this research, however, has focused on trying to find the most effective methods of proficiency detection among various machine learning algorithms or the most influential features of input text for classifying proficiency level. The experiments performed in these papers do not focus on the possible uses of existing language models, such as Google's Gemini or Meta's Llama for this task. Models such as Gemini are backed by massive amounts of training data and sources, which, when combined with their ease of access and ease of use, means that they could be very effective and convenient for the task at hand. Models like Llama also utilize large training datasets but are very lightweight and can even be downloaded and run locally, which means that if their performance is anywhere near as good as the likes of Gemini, they could prove incredibly useful for off-the-grid or security-focused use cases. Additionally, prior research has not looked into the association between sentences that cause human classification disagreements and those that are more difficult for language models. In our experiment, we prompted Gemini and multiple different versions of Llama to attempt to classify 10000 English input sentences by their CEFR level and used the pre-existing labels from 2 human annotators to derive a number of data points regarding accuracy, comparisons between LLM and human annotators, and both fine-grained and coarse-grained level discrimination accuracies. Conclusions were derived from these results pertaining to the efficacy of generalized language models for this task along with the drawbacks of such an approach.
<p>

<h3 id="motivation">Motivation </h3> 
<p>
The primary impact of this experiment will be assessing the feasibility of using out-of-the-box language models for language proficiency assessment. If commonly available language models like ChatGPT and Llama1B are found to be effective, they could prove to be very useful for professionals in the fields mentioned above hoping to streamline their workflow. Even if the models are found to only be partially reliable, they could still be used to identify outlier sentences that the human annotators would then not need to spend time assessing. This could cause drastic increases in efficiency for the field of language proficiency detection. Additionally, if the models are found to be very effective, then they could theoretically be used as more consistent replacements for human annotators, since they would be more impartial and less susceptible to issues stemming from individual interpretation of the CEFR scale.
</p>

<hr>

<h2 id="problemform_ideas">Problem Formulation/Proposed Ideas</h2>

<p>
<b>Problem Formulation</b>
</p>

<p>
The primary challenge is to accurately assess language proficiency across the CEFR proficiency scale (A1–C2), which requires distinguishing both between and within levels. 
  Existing models are effective at broader classification but lack the precision needed for adjacent level distinctions, and their performance is largely model-size dependent. 
  This project investigates two main gaps: 1) Existing language models' abilities to classify proficiency levels accurately, and 2) How various factors—model type, prompt design, and text features—affect performance.
</p>

<p>
<b>Proposed Ideas</b>
</p>

<p>To address these challenges, we propose:</p>

<ol>
    <li><strong>Model comparisons</strong>
        <ul>
            <li>Evaluate direct accuracy, ±1 level accuracy, and above/below accuracy.</li>
            <li>This will include models of various sizes, specifically Llama models (Llama 3.2 1B, Llama 3.2 3B, Llama 3.1 8B) and ChatGPT.</li>
            <li>We will investigate if higher model size and complexity correspond to improved accuracy at both coarse-grained and fine-grained levels.</li>
        </ul>
    </li>

    <li><strong>Exploring different prompting techniques:</strong>
        <ul>
            <li>Apply prompting strategies such as few-shot learning with retrieval-augmented generation (RAG) and chain-of-thought reasoning.</li>
            <li>Assess how prompt design impacts model performance on fine-grained distinctions.</li>
        </ul>
    </li>

    <li><strong>Fine-Tuning (Optional):</strong>
        <ul>
            <li>If time allows, experiment with fine-tuning select models on CEFR-labeled datasets to explore potential improvements in differentiating closely spaced levels.</li>
            <li>Fine-tuning would not be a primary focus but could help assess the extent to which tailored training enhances proficiency classification.</li>
        </ul>
    </li>
</ol>

<!-- PLEASE UNCOMMENT THIS IF NEEDED [STARTING HERE]
<hr>    
    
<h2 id="problemform_ideas">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.
</p> [ENDING HERE] -->

<hr>    

<h2 id="approach">Preliminary result</h2>

<p>ChatGPT-3 exhibited wrong estimation, such as estimating a question to be more difficult for C1-level learners (i.e., “Proficient”) than A1-level learners (i.e., “Basic”) <a href="https://arxiv.org/pdf/2307.08393">[1]</a>.</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
  </tbody>
  <caption>Table 1. Comparison of A1-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">C1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
  </tbody>
  <caption>Table 2. Comparison of A2-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">C1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A1</td>
    </tr>
  </tbody>
  <caption>Table 3. Comparison of B1-level classifications across Llama models and prompting techniques.</caption>
</table>

<h4>Error analysis of model performance in CEFR level classification</h4>

<ul>
    <li><strong>Llama 3.2 1B</strong> achieved an overall accuracy of <strong>75%</strong> for B1 sentence. This model frequently overrated A1-level sentences as A2 or B2. However, it correctly predicted B1-level sentence, despite the small size of the model.</li>

    <li><strong>Llama 3.2 3B</strong> demonstrated some variability in its performance. This model often rated A1-level sentences as A2 or B1. Additionally, it frequently classified A2-level sentences as B1, B2, or even C1. There were instances where B1 sentences were underrated and classified as A1 when using system prompting. This model highlighted critical inconsistencies in its classifications.</li>

    <li><strong>Llama 3.1 8B</strong> displayed the most diverse classification outcomes, which resulted in the greatest variability in performance. This model consistently rated A1-level sentences as A2 or B2 and often classified A2-level sentences as B1 or B2. Furthermore, it varied in its classification of B1 sentences, with some being downgraded to A1 or A2 while others were classified as C1 or B2. This model showcased a lack of stability in its predictions, even though it has the biggest size across 3 models.</li>
</ul>
<hr>

    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div>
<br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
    
<h3 id="replicability">Replicability</h2>
<p>Our results are extremely replicable. Any dataset of CEFR labeled English text combined with any prompt could be relatively easily run through ChatGPT and Llama models to get similarly formatted results to analyze. Of course the results would likely be different for different datasets, models, and prompts, but that would likely be due more to the annotators used for the dataset and the prompts used, and less to the models themselves. This is once again because the CEFR scale is open to human interpretation.</p>

<h3 id="dataset">Dataset</h2>
<p>Our dataset was large enough to be significant, having about 10000 labeled sentences. The main drawback of our dataset was the use of only two human annotators, who often disagreed. A more fair dataset would have more annotators so a more definitive score could be determined for each sentence. This could have led to many different results, since often the models agreed with the annotator who labeled a sentence at a lower CEFR level, and perhaps with more annotators the true label would have trended down towards that lower level, in which case the model would have had a higher accuracy. Another possible drawback of our dataset is a bias towards higher level labels for longer sentences, which could be a cause for our result that the smaller Llama1B model was the most effective classifier. Our dataset choice did not seem to have an affect on any other people's choice in project to undertake, as our project was quite unique in its goal of testing already-existing language models for this task.</p>

<h3 id="ethics">Ethics</h2>
<p>The only possible risk this work could pose to society is based on the perceived morality of using machine learning for a task as "human" and language proficiency assessment. While we have mentioned that human annotators can be inconsistent at classifying text on the CEFR scale, and that language models could potentially solve this inconsistency problem, there is an argument to be made that only a human should be classifying human-written text. This mainly stems from the fact that most applications of this task are used in very high-importance fields like education, immigration, and work authorizations, all of which can fundamentally change a person's life. For this reason, it is understandable to argue that even a "mostly accurate" model should never be used for this task, since the highest possible accuracy is needed to ensure people's futures are not put at stake due to a false classification. The only real ways to combat this issue are to either develop models that exceedingly accurate in its assessments, or to have a human annotator evaluate the predictions of the model, which defeats the main purpose of using language models for this task in the first place.</p>

<h3 id="futurework">Future Work</h2>
<p>As the results show (\ref{fig:1}), our smallest model, Llama1B, was most effective at both correctly classifying and being within 1 CEFR level of the true label of the sentences, while ChatGPT was the second most effective, and Llama3B and Llama8B were both very ineffective. Additionally, ChatGPT and Llama1B were found to both retain accuracy when focusing on the more subjective sentences for which the human annotators of our disagreed, while Llama3B and Llama8B's accuracies went down significantly in this case. This is quite surprising, and more research could be done into the reasons for this behavior if this behavior continues for other models with differing sizes.</p>

<p>Overall, our best exact accuracy was found to be about 45\%, and our best within-1-level accuracy was about 90\%. These values are lower than previous research into specialized models and methods for language proficiency assessment, which achieved exact accuracies as high as 82\% on the CEFR scale. This is to be expected, since the models were not fine-tuned or trained, and instead were used as-is. In the future, more research could be done on the effectiveness of fine-tuned or pre-trained open-source models for this task, to see if they can surpass custom models made for this task.</p>

<p>Another key result is that the models with lower accuracies, specifically Llama3B and Llama8B, were consistently classifying sentences lower than they actually were on the CEFR scale, whereas higher performing models were more evenly split between high and low classifications. A better version of this experiment could have attempted to correct for this by shifting the predicted scores from Llama3B and Llama8B up by a set amount to account for this bias towards lower CEFR scores.</p>

<p>Other ideas for future research include assessing the models' abilities in discriminating between closer and further apart CEFR level sentences, testing the models' efficacy for discriminating between broader and more specific scales (i.e. CEFR with just A, B, and C vs CEFR with A1, A1.5, A2, ...), and more research into which prompting techniques are most effective for this task, since the differing prompting techniques used in our experiment had no clear effects, with each of them being better in some cases and worse in others.</p>


<hr>


  </div>
  


</body></html>
