<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">English Proficiency Detection and Generation</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team Mosaic Pineapple</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              Nhi Dang
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Riana Hoagland
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              Harrison Wallander
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Language proficiency assessment using the CEFR scale is traditionally accomplished by human language experts, but this task's subjective nature leads to inconsistencies in assessment results due to differing views of sentence complexity between experts. This has led to interest in research regarding the feasibility of using various machine learning methods and models for proficiency assessment. While other papers have focused on customized and/or fine-tuned methods for this task, and have found that certain algorithms and models can be optimized to be very accurate, we sought to find out how effective already existing open-source models like OpenAI's ChatGPT and Meta's Llama are at this task. We chose this approach because having commonly-available and consistent methods for automatic proficiency assessment could prove very useful in a number of fields including education, immigration, and work authorization. 

To this end, we devised an experiment in which we used 3 set prompts in conjunction with 10,000 CEFR-labeled English sentences to test the effectiveness of ChatGPT, Llama1B, Llama3B, and Llama8B at the task of language proficiency. Our main findings are that both ChatGPT and Llama1B approach 90\% accuracy in classifying sentences within 1 CEFR level of their true level, while Llama3B and Llama8B tend to lag behind. Additionally, we found that the better a model is at classifying overall, the more consistent it is at classifying more "subjective" sentences according to experts. This means that both ChatGPT and Llama1B could be effectively used for automatic proficiency detection tasks, and that their results could be trusted to be both somewhat accurate, and more importantly, consistent.</p>

<hr>

<h2 id="teaser">Overview </h2>

<img src="https://raw.githubusercontent.com/RianaH/MosaicPineapple/main/one.jpg" alt="Process Overview" width="600" height="400">

<hr>

<h2 id="introduction">Introduction </h2>

<h3 id="problemoverview">Problem Overview </h3>    
<p>
Language proficiency assessment is a critical task with a wide range of uses in education, immigration, and work authorization. One of the most common methods for classifying a person's proficiency in a language is the Common European Framework of Reference for Languages (CEFR) scale, which assigns a person's proficiency in a given language to one of 6 levels (A1, A2, B1, B2, C1, and C2) in increasing order of proficiency. These levels are defined by a number of different aspects of the language to be classified, including vocabulary, sentence structure, and grammar. To some extent, this scale is inherently dependent on the interpretation of the person who classifies each sample, which often leads to situations in which two people may assign the same sample of a language to different levels on the CEFR scale. This has lead to a desire in recent years for an automatic language proficiency assessment, which would be unbiased and able to fully standardize the act of classifying a person's proficiency in a given language.</p>
<p>

<h3 id="background">Background </h3> 
<p>
Significant research has already been done into the topic of using various machine learning techniques to classify text by proficiency level. The majority of this research, however, has focused on trying to find the most effective methods of proficiency detection among various machine learning algorithms or the most influential features of input text for classifying proficiency level. The experiments performed in these papers do not focus on the possible uses of existing language models, such as Google's Gemini or Meta's Llama for this task. Models such as Gemini are backed by massive amounts of training data and sources, which, when combined with their ease of access and ease of use, means that they could be very effective and convenient for the task at hand. Models like Llama also utilize large training datasets but are very lightweight and can even be downloaded and run locally, which means that if their performance is anywhere near as good as the likes of Gemini, they could prove incredibly useful for off-the-grid or security-focused use cases. Additionally, prior research has not looked into the association between sentences that cause human classification disagreements and those that are more difficult for language models. In our experiment, we prompted Gemini and multiple different versions of Llama to attempt to classify 10000 English input sentences by their CEFR level and used the pre-existing labels from 2 human annotators to derive a number of data points regarding accuracy, comparisons between LLM and human annotators, and both fine-grained and coarse-grained level discrimination accuracies. Conclusions were derived from these results pertaining to the efficacy of generalized language models for this task along with the drawbacks of such an approach.
<p>

<h3 id="motivation">Motivation </h3> 
<p>
The primary impact of this experiment will be assessing the feasibility of using out-of-the-box language models for language proficiency assessment. If commonly available language models like ChatGPT and Llama1B are found to be effective, they could prove to be very useful for professionals in the fields mentioned above hoping to streamline their workflow. Even if the models are found to only be partially reliable, they could still be used to identify outlier sentences that the human annotators would then not need to spend time assessing. This could cause drastic increases in efficiency for the field of language proficiency detection. Additionally, if the models are found to be very effective, then they could theoretically be used as more consistent replacements for human annotators, since they would be more impartial and less susceptible to issues stemming from individual interpretation of the CEFR scale.
</p>

<hr>

<h2 id="problemform_ideas">Problem Formulation/Proposed Ideas</h2>

<p>
<b>Problem Formulation</b>
</p>

<p>
The primary challenge is to accurately assess language proficiency across the CEFR proficiency scale (A1–C2), which requires distinguishing both between and within levels. 
  Existing models are effective at broader classification but lack the precision needed for adjacent level distinctions, and their performance is largely model-size dependent. 
  This project investigates two main gaps: 1) Existing language models' abilities to classify proficiency levels accurately, and 2) How various factors—model type, prompt design, and text features—affect performance.
</p>

<p>
<b>Proposed Ideas</b>
</p>

<p>To address these challenges, we propose:</p>

<ol>
    <li><strong>Model comparisons</strong>
        <ul>
            <li>Evaluate direct accuracy, ±1 level accuracy, and above/below accuracy.</li>
            <li>This will include models of various sizes, specifically Llama models (Llama 3.2 1B, Llama 3.2 3B, Llama 3.1 8B) and ChatGPT.</li>
            <li>We will investigate if higher model size and complexity correspond to improved accuracy at both coarse-grained and fine-grained levels.</li>
        </ul>
    </li>

    <li><strong>Exploring different prompting techniques:</strong>
        <ul>
            <li>Apply prompting strategies such as few-shot learning with retrieval-augmented generation (RAG) and chain-of-thought reasoning.</li>
            <li>Assess how prompt design impacts model performance on fine-grained distinctions.</li>
        </ul>
    </li>

    <li><strong>Fine-Tuning (Optional):</strong>
        <ul>
            <li>If time allows, experiment with fine-tuning select models on CEFR-labeled datasets to explore potential improvements in differentiating closely spaced levels.</li>
            <li>Fine-tuning would not be a primary focus but could help assess the extent to which tailored training enhances proficiency classification.</li>
        </ul>
    </li>
</ol>

<!-- PLEASE UNCOMMENT THIS IF NEEDED [STARTING HERE]
<hr>    
    
<h2 id="problemform_ideas">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.
</p> [ENDING HERE] -->

<hr>    

<h2 id="approach">Preliminary result</h2>

<p>ChatGPT-3 exhibited wrong estimation, such as estimating a question to be more difficult for C1-level learners (i.e., “Proficient”) than A1-level learners (i.e., “Basic”) <a href="https://arxiv.org/pdf/2307.08393">[1]</a>.</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
  </tbody>
  <caption>Table 1. Comparison of A1-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">C1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
  </tbody>
  <caption>Table 2. Comparison of A2-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">C1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A1</td>
    </tr>
  </tbody>
  <caption>Table 3. Comparison of B1-level classifications across Llama models and prompting techniques.</caption>
</table>

<h4>Error analysis of model performance in CEFR level classification</h4>

<ul>
    <li><strong>Llama 3.2 1B</strong> achieved an overall accuracy of <strong>75%</strong> for B1 sentence. This model frequently overrated A1-level sentences as A2 or B2. However, it correctly predicted B1-level sentence, despite the small size of the model.</li>

    <li><strong>Llama 3.2 3B</strong> demonstrated some variability in its performance. This model often rated A1-level sentences as A2 or B1. Additionally, it frequently classified A2-level sentences as B1, B2, or even C1. There were instances where B1 sentences were underrated and classified as A1 when using system prompting. This model highlighted critical inconsistencies in its classifications.</li>

    <li><strong>Llama 3.1 8B</strong> displayed the most diverse classification outcomes, which resulted in the greatest variability in performance. This model consistently rated A1-level sentences as A2 or B2 and often classified A2-level sentences as B1 or B2. Furthermore, it varied in its classification of B1 sentences, with some being downgraded to A1 or A2 while others were classified as C1 or B2. This model showcased a lack of stability in its predictions, even though it has the biggest size across 3 models.</li>
</ul>
<hr>

    
<h2 id="results">Results</h2>
<p>
<b>
  
 We tested the performance of 9 Llama models and the free version of ChatGPT. The Llama models tested were the ones of size 1 billion to 8 billion variables. For all of them, we measured accuracy in sentence classification ability across three different prompting techniques (0-shot, 1-shot, chain-of-thought). A correct classification was defined as when the model predicted the true database label. For incorrect guesses, we measured many factors such as how many levels off the guess was, whether the guess was too high or too low and whether there was a correlation between the probability of incorrect guessing and annotator disagreement (sentence "subjectivity"). The research of our analysis is presented below.Note that 1-shot and few-shot are used interchangeably throughout this report.

Figure 1 addresses the research question of how different sizes models fair on sentence classification. The percentage of correct answers is graphed in blue, where correct was defined as a label that matched the true label in the CEFR database for each sentence. Surprisingly, smaller-sized models outperformed larger-sized models in correctness. Three of the four best-performing models for correctness were the three Llama 1 billion models. The other one was ChatGPT, meaning our results also show that ChatGPT performed quite well on sentence classification, compared to the other models tested. 
We additionally investigated how different prompting techniques performed. The different prompting techniques produced different results across the different models. For example, for Llama 1B, few-shot performed the best, then 0-shot, then COT. However, for the Llama 3B, 0-shot was the best, then COT, then few-shot. Finally, for the Llama 8B, 0-shot was the best, and few-shot and COT were nearly tied. Overall, we observe that a single prompting technique does not outperform the others reliably. 

We also plotted the answers that were one-off - either one too high or one too low - which are graphed in orange. Two-off, three-off, and four-off classifications are graphed as well. The fact that the orange bars are above the blue bars for all but one model (Llama 1B 1-shot) shows that the classification accuracy of the models is generally not the best. However, the orange and blue bars are much higher than the other bars for the smaller models, showing that the models are at least close in their classifications of sentence levels. For the larger models, this isn't necessarily the case. For Llama 8, the percentage of two-off guesses was almost as high as the percentage of correct guesses, especially on 1-shot and COT. This is true for Llama 3 COT as well, and for Llama 3 1-shot the green bar is higher than the blue bar. The red and purple bars are generally low, with the highest being the Llama 3B 1-shot, which is the worst-performing model and prompt combination with a low blue bar, and high green and red bars compared to the other models. 

One prominent failure case for one level off is the misclassification of a C1 sentence as B2, for example, "The standard shape for pyrohy is oblong with tapering ends, but rectangular or circular pyrohy are also common." This is due to the misclassification of the vocabulary "pyrohy" as a B2 rather than a C1 word. We also observe failure cases for two level off as all models misclassify a B1 sentence as A1 "My brother is my favorite sibling." Three-off failure examples include misclassifying a C2 sentence as B1 "With approximately 25 phonemes, the German consonant system has an average number of consonants in comparison with other languages." These failure cases might stem from the nuance nature of CEFR language in terms of coherence, fluency and context. This may be resolved by adding more instructions to our prompts to focus more on these topics. We also observe some hallucination by rating an A2 sentence as C2 by Llama 3.2-1B. 

ChatGPT also overperforms Llama 3.2-1B in correctly predicting C1 sentences, which are underrated by Llama as B1-B2, for example, "Despite their failure to score any hits, the American torpedo attacks achieved three important results." The main difference in reasoning between these two models is that ChatGPT could capture the complex sentence structure ("despite") and abstract vocabulary ("failure, achieved") that Llama3.2-1B fails to mention. There were also failure cases of Llama 3.2-3B and Llama 3.1-8B underrating sentences while Llama 3.2-1B correctly classifies it; for example, "Like most rattlesnakes, they live on land, and are not good climbers ." is B2 but rated as A2-B1. Surprisingly, the reasoning for these models are very similar to each other.

Overall, we were able to successfully show that the size of the model does matter in sentence classification accuracy and that the prompting technique does not appear to predict accuracy across models. 

Figure 2 shows how far off each model was on average and provides another way, in addition to \textbf{Figure 1} to analyze the best and worst performing models. Again, ChatGPT and the Llama 1B 0-shot and 1-shot perform the best. However, this graph allows us to see that Llama 8B 0-shot performs the fourth best by this metric, instead of the Llama 1B 1-shot. This graph also suggests that the LLama 8Bs performed better than the Llama 3Bs on average. This suggests that it is not a general trend that smaller models perform better but is instead that LLama 1B performs better than larger models. The fact that Llama 3B performs the worst warrants more research into why this is the case. It would be interesting to expand the test to other sizes of models to see what pattern emerges.

While there was not a strong prompting-based pattern in the first graph, this graph suggests that 0-shot performs the best, as it is the smallest bar for each of the Llama models. This is surprising, as one might guess that the more you guide a model, the better it performs. However, this suggests that the models perform best without guidance/examples. 

Reconciling the results of the first and second graphs, depends on what one cares about most. The first graph is more helpful if you care primarily about the number of true correct guesses and want to base performance on that. Or if you want to avoid the worst performance - e.g. maybe you can avoid any model with a high bar for 4 off. The second graph is more helpful if you care about overall performance and want a measure of which models tend to be more or less off on across all answers. Taking both graphs into consideration together is probably the best approach to identifying the best-performing models.

This figure, similar to the first figure, was a success in showing that models perform differently on language-level classification tasks and that the size of the model does matter in classification accuracy. This figure also shows us that it turns out the prompting technique does matter as well, which we were not able to glean from the first figure.

Figure 3 explores the research question of whether there is a bias towards predicting too high or too low when incorrect classifications are made. This question gets at whether models have a propensity to think sentences are overly proficient or under-performing. We graph the percent of correct answers alongside the perfect of times each model classified sentences too high in level versus too low in level. Overall, there was a tendency for models to guess too low, meaning they unclassified the proficiency of sentences; every green bar is above its corresponding orange bar. This bias was less present in the better performing models - with it being a near tie for ChatGPT and Llama 1B 0-shot - and most pronounced in the worst performing models (namely Llama 3B 1-shot). The higher the relative size of the green bar to the orange bar, the higher the model's bias towards guessing low. 

Overall, we successfully showed that there is a bias for models to under-predict sentence level and that this bias is strongest in the worst-performing models. This suggests that much of the issue of model performance overall is being driven by an under-performance bias. Hence, one way to improve LLM's ability to correctly assess language level may be to introduce methods to promote higher classification. This could involve things like training on a higher subset of more sophisticated sentences or introducing a small-weight term penalizing under-classification. It's possible prompting techniques could also help address this - perhaps including in the prompt a suggested check such as "Please double check that you did not classify the sentence too low". As we learned in class, simply asking models to think step by step or check their answers often results in more accurate answers, so prompting the model to make sure it's not under-guessing could be a useful strategy to test. 

Some failure cases of guessing too low or too high include classifying C1-C2 sentences as B1-B2 (for example, "Modern didgeridoo designs are distinct from the traditional Australian Aboriginal didgeridoo, and are innovations recognized by musicologists.", or vice-versa (for example, "Whitbourne has many of the facilities of a small town that has traditionally been a regional service center.").

  
</b>
</p>
<p>
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div>
<br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
    
<h3 id="replicability">Replicability</h2>
<p>Our results are extremely replicable. Any dataset of CEFR labeled English text combined with any prompt could be relatively easily run through ChatGPT and Llama models to get similarly formatted results to analyze. Of course the results would likely be different for different datasets, models, and prompts, but that would likely be due more to the annotators used for the dataset and the prompts used, and less to the models themselves. This is once again because the CEFR scale is open to human interpretation.</p>

<h3 id="dataset">Dataset</h2>
<p>Our dataset was large enough to be significant, having about 10000 labeled sentences. The main drawback of our dataset was the use of only two human annotators, who often disagreed. A more fair dataset would have more annotators so a more definitive score could be determined for each sentence. This could have led to many different results, since often the models agreed with the annotator who labeled a sentence at a lower CEFR level, and perhaps with more annotators the true label would have trended down towards that lower level, in which case the model would have had a higher accuracy. Another possible drawback of our dataset is a bias towards higher level labels for longer sentences, which could be a cause for our result that the smaller Llama1B model was the most effective classifier. Our dataset choice did not seem to have an affect on any other people's choice in project to undertake, as our project was quite unique in its goal of testing already-existing language models for this task.</p>

<h3 id="ethics">Ethics</h2>
<p>The only possible risk this work could pose to society is based on the perceived morality of using machine learning for a task as "human" and language proficiency assessment. While we have mentioned that human annotators can be inconsistent at classifying text on the CEFR scale, and that language models could potentially solve this inconsistency problem, there is an argument to be made that only a human should be classifying human-written text. This mainly stems from the fact that most applications of this task are used in very high-importance fields like education, immigration, and work authorizations, all of which can fundamentally change a person's life. For this reason, it is understandable to argue that even a "mostly accurate" model should never be used for this task, since the highest possible accuracy is needed to ensure people's futures are not put at stake due to a false classification. The only real ways to combat this issue are to either develop models that exceedingly accurate in its assessments, or to have a human annotator evaluate the predictions of the model, which defeats the main purpose of using language models for this task in the first place.</p>

<h3 id="futurework">Future Work</h2>
<p>As the results show (\ref{fig:1}), our smallest model, Llama1B, was most effective at both correctly classifying and being within 1 CEFR level of the true label of the sentences, while ChatGPT was the second most effective, and Llama3B and Llama8B were both very ineffective. Additionally, ChatGPT and Llama1B were found to both retain accuracy when focusing on the more subjective sentences for which the human annotators of our disagreed, while Llama3B and Llama8B's accuracies went down significantly in this case. This is quite surprising, and more research could be done into the reasons for this behavior if this behavior continues for other models with differing sizes.</p>

<p>Overall, our best exact accuracy was found to be about 45\%, and our best within-1-level accuracy was about 90\%. These values are lower than previous research into specialized models and methods for language proficiency assessment, which achieved exact accuracies as high as 82\% on the CEFR scale. This is to be expected, since the models were not fine-tuned or trained, and instead were used as-is. In the future, more research could be done on the effectiveness of fine-tuned or pre-trained open-source models for this task, to see if they can surpass custom models made for this task.</p>

<p>Another key result is that the models with lower accuracies, specifically Llama3B and Llama8B, were consistently classifying sentences lower than they actually were on the CEFR scale, whereas higher performing models were more evenly split between high and low classifications. A better version of this experiment could have attempted to correct for this by shifting the predicted scores from Llama3B and Llama8B up by a set amount to account for this bias towards lower CEFR scores.</p>

<p>Other ideas for future research include assessing the models' abilities in discriminating between closer and further apart CEFR level sentences, testing the models' efficacy for discriminating between broader and more specific scales (i.e. CEFR with just A, B, and C vs CEFR with A1, A1.5, A2, ...), and more research into which prompting techniques are most effective for this task, since the differing prompting techniques used in our experiment had no clear effects, with each of them being better in some cases and worse in others.</p>


<hr>


  </div>
  


</body></html>
