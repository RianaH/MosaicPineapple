<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Comparing Language Proficiency Classification of Language Models</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team Mosaic Pineapple</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              Nhi Dang
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Riana Hoagland
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              Harrison Wallander
          </p>
        </div>
        
      </div>

      <br/>
    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Language proficiency assessment using the CEFR scale is traditionally accomplished by human language experts, but this task's subjective nature leads to inconsistencies in assessment results due to differing views of sentence complexity between experts. This has led to interest in research regarding the feasibility of using various machine learning methods and models for proficiency assessment. While other papers have focused on customized and/or fine-tuned methods for this task, and have found that certain algorithms and models can be optimized to be very accurate, we sought to find out how effective already existing open-source models like OpenAI's ChatGPT and Meta's Llama are at this task. We chose this approach because having commonly-available and consistent methods for automatic proficiency assessment could prove very useful in a number of fields including education, immigration, and work authorization. 

To this end, we devised an experiment in which we used 3 set prompts in conjunction with 10,000 CEFR-labeled English sentences to test the effectiveness of ChatGPT, Llama1B, Llama3B, and Llama8B at the task of language proficiency. Our main findings are that both ChatGPT and Llama1B approach 90% accuracy in classifying sentences within 1 CEFR level of their true level, while Llama3B and Llama8B tend to lag behind. Additionally, we found that the better a model is at classifying overall, the more consistent it is at classifying more "subjective" sentences according to experts. This means that both ChatGPT and Llama1B could be effectively used for automatic proficiency detection tasks, and that their results could be trusted to be both somewhat accurate, and more importantly, consistent.</p>

<hr>

<h2 id="teaser">Overview </h2>

<img src="https://raw.githubusercontent.com/RianaH/MosaicPineapple/main/one.jpg" alt="Process Overview" width="600" height="400">

<hr>

<h2 id="introduction">Introduction </h2>

<h3 id="problemoverview">Problem Overview </h3>    
<p>
Language proficiency assessment is a critical task with a wide range of uses in education, immigration, and work authorization. One of the most common methods for classifying a person's proficiency in a language is the Common European Framework of Reference for Languages (CEFR) scale, which assigns a person's proficiency in a given language to one of 6 levels (A1, A2, B1, B2, C1, and C2) in increasing order of proficiency. These levels are defined by a number of different aspects of the language to be classified, including vocabulary, sentence structure, and grammar. To some extent, this scale is inherently dependent on the interpretation of the person who classifies each sample, which often leads to situations in which two people may assign the same sample of a language to different levels on the CEFR scale. This has lead to a desire in recent years for an automatic language proficiency assessment, which would be unbiased and able to fully standardize the act of classifying a person's proficiency in a given language.</p>
<p>

<h3 id="background">Background </h3> 
<p>
Significant research has already been done into the topic of using various machine learning techniques to classify text by proficiency level. The majority of this research, however, has focused on trying to find the most effective methods of proficiency detection among various machine learning algorithms or the most influential features of input text for classifying proficiency level. The experiments performed in these papers do not focus on the possible uses of existing language models, such as Google's Gemini or Meta's Llama for this task. Models such as Gemini are backed by massive amounts of training data and sources, which, when combined with their ease of access and ease of use, means that they could be very effective and convenient for the task at hand. Models like Llama also utilize large training datasets but are very lightweight and can even be downloaded and run locally, which means that if their performance is anywhere near as good as the likes of Gemini, they could prove incredibly useful for off-the-grid or security-focused use cases. Additionally, prior research has not looked into the association between sentences that cause human classification disagreements and those that are more difficult for language models. In our experiment, we prompted Gemini and multiple different versions of Llama to attempt to classify 10000 English input sentences by their CEFR level and used the pre-existing labels from 2 human annotators to derive a number of data points regarding accuracy, comparisons between LLM and human annotators, and both fine-grained and coarse-grained level discrimination accuracies. Conclusions were derived from these results pertaining to the efficacy of generalized language models for this task along with the drawbacks of such an approach.
<p>

<h3 id="motivation">Motivation </h3> 
<p>
The primary impact of this experiment will be assessing the feasibility of using out-of-the-box language models for language proficiency assessment. If commonly available language models like ChatGPT and Llama1B are found to be effective, they could prove to be very useful for professionals in the fields mentioned above hoping to streamline their workflow. Even if the models are found to only be partially reliable, they could still be used to identify outlier sentences that the human annotators would then not need to spend time assessing. This could cause drastic increases in efficiency for the field of language proficiency detection. Additionally, if the models are found to be very effective, then they could theoretically be used as more consistent replacements for human annotators, since they would be more impartial and less susceptible to issues stemming from individual interpretation of the CEFR scale.
</p>

<hr>

<h2 id="problemform_ideas">Problem Formulation/Proposed Ideas</h2>

<p>
<b>Problem Formulation</b>
</p>

<p>
The primary challenge is to accurately assess language proficiency across the CEFR proficiency scale (A1–C2), which requires distinguishing both between and within levels. 
  Existing models are effective at broader classification but lack the precision needed for adjacent level distinctions, and their performance is largely model-size dependent. 
  This project investigates two main gaps: 1) Existing language models' abilities to classify proficiency levels accurately, and 2) How various factors—model type, prompt design, and text features—affect performance.
</p>

<p>
<b>Proposed Ideas</b>
</p>

<p>To address these challenges, we propose:</p>

<ol>
    <li><strong>Model comparisons</strong>
        <ul>
            <li>Evaluate direct accuracy, ±1 level accuracy, and above/below accuracy.</li>
            <li>This will include models of various sizes, specifically Llama models (Llama 3.2 1B, Llama 3.2 3B, Llama 3.1 8B) and ChatGPT.</li>
            <li>We will investigate if higher model size and complexity correspond to improved accuracy at both coarse-grained and fine-grained levels.</li>
        </ul>
    </li>

    <li><strong>Exploring different prompting techniques:</strong>
        <ul>
            <li>Apply prompting strategies such as few-shot learning with retrieval-augmented generation (RAG) and chain-of-thought reasoning.</li>
            <li>Assess how prompt design impacts model performance on fine-grained distinctions.</li>
        </ul>
    </li>

    <li><strong>Fine-Tuning (Optional):</strong>
        <ul>
            <li>If time allows, experiment with fine-tuning select models on CEFR-labeled datasets to explore potential improvements in differentiating closely spaced levels.</li>
            <li>Fine-tuning would not be a primary focus but could help assess the extent to which tailored training enhances proficiency classification.</li>
        </ul>
    </li>
</ol>

<h2 id="problemform_ideas">Approach</h2>

<p>
Dataset: We use CEFR-SP dataset, which provides 17000 English sentences annotated with
Common European Framework of Reference (CEFR) scale (A1-C2) levels assigned by English-education professionals. These
sentences serve as ground truth for evaluating model performance. Each sentence is annotated with CEFR proficiency
levels (A1-C2) by 2 annotators.
</p>

<p>
Models and Prompting techniques: We aim to compare fine-grained models with varying sizes of Llama 3 family: 
Llama 3.2-1B, Llama 3.2-3B, Llama 3.1-8B. The models are available on Hugging Face's meta-llama. We also use ChatGPT-4o 
as the baseline for our analysis. Due to the rate limit (restrictions that providers' API imposes on the number of times
a user or client can access services within a specified period of time) from Llama3 and OpenAI, the language classification 
tasks are run by batches of 100 sentences.
</p>

<p>

We also explore various prompting strategies to assess their impact on model performance. For zero-shot prompting,
models are provided with the sentence and a brief explanation of CEFR level and no examples: "Classify each
sentence based on linguistic complexity, vocabulary, grammar, coherence, context, and overall fluency, assign CEFR
levels: A1, A2, B1, B2, C1, or C2".
</p>

<p>
For few-shot promptings, models are supplied with 6 labeled examples from A1 to C2 to guide classification.
For Chain-of-thought (CoT) prompting, models are guided to reason through the classification process step-by-step:
"Think step by step. Analyze the following sentences considering their vocabulary, grammar, coherence, context
and overall fluency. Step 1: proceed to ask guiding questions about vocabulary ...". All prompts are invoked to
models using Hugging Face and LangChain frameworks to standardize prompt formatting and ensure reproducibility.
</p>

<p>
Classification and Analysis Pipeline: The classification pipeline consists of the following steps:
Each sentence is classified by all selected models, using all three prompting techniques (zero-shot, few-shot, chain-of-thought). 
Classification results are compared against the pre-labeled dataset. When the annotators' labels
disagree, we use the higher label according to previous research. For each model and prompting technique, we
calculate the proportion of correct classification, the proportion of classifications within one/two/three level(s) of
the ground truth, the difference in accuracy across CEFR levels A1-C2, the difference in accuracy between sentences
with different annotation and that of same annotation, the calibration for the classification for each model.
</p>
<hr>    

<h2 id="approach">Preliminary result</h2>

<p>ChatGPT-3 exhibited wrong estimation, such as estimating a question to be more difficult for C1-level learners (i.e., “Proficient”) than A1-level learners (i.e., “Basic”) <a href="https://arxiv.org/pdf/2307.08393">[1]</a>.</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
  </tbody>
  <caption>Table 1. Comparison of A1-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">C1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
  </tbody>
  <caption>Table 2. Comparison of A2-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">C1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A1</td>
    </tr>
  </tbody>
  <caption>Table 3. Comparison of B1-level classifications across Llama models and prompting techniques.</caption>
</table>

<h4>Error analysis of model performance in CEFR level classification</h4>

<ul>
    <li><strong>Llama 3.2 1B</strong> achieved an overall accuracy of <strong>75%</strong> for B1 sentence. This model frequently overrated A1-level sentences as A2 or B2. However, it correctly predicted B1-level sentence, despite the small size of the model.</li>

    <li><strong>Llama 3.2 3B</strong> demonstrated some variability in its performance. This model often rated A1-level sentences as A2 or B1. Additionally, it frequently classified A2-level sentences as B1, B2, or even C1. There were instances where B1 sentences were underrated and classified as A1 when using system prompting. This model highlighted critical inconsistencies in its classifications.</li>

    <li><strong>Llama 3.1 8B</strong> displayed the most diverse classification outcomes, which resulted in the greatest variability in performance. This model consistently rated A1-level sentences as A2 or B2 and often classified A2-level sentences as B1 or B2. Furthermore, it varied in its classification of B1 sentences, with some being downgraded to A1 or A2 while others were classified as C1 or B2. This model showcased a lack of stability in its predictions, even though it has the biggest size across 3 models.</li>
</ul>
<hr>

    
<h2 id="results">Results</h2>
<p>
We tested the performance of 9 Llama models and the free version of ChatGPT. The Llama models tested were the ones of size 1 billion to 8 billion variables. For all of them, we measured accuracy in sentence classification ability across three different prompting techniques (0-shot, 1-shot, chain-of-thought). A correct classification was defined as when the model predicted the true database label. For incorrect guesses, we measured many factors such as how many levels off the guess was, whether the guess was too high or too low and whether there was a correlation between the probability of incorrect guessing and annotator disagreement (sentence "subjectivity"). The research of our analysis is presented below.Note that 1-shot and few-shot are used interchangeably throughout this report.
</p>
<p>
Figure 1 addresses the research question of how different sizes models fair on sentence classification. The percentage of correct answers is graphed in blue, where correct was defined as a label that matched the true label in the CEFR database for each sentence. Surprisingly, smaller-sized models outperformed larger-sized models in correctness. Three of the four best-performing models for correctness were the three Llama 1 billion models. The other one was ChatGPT, meaning our results also show that ChatGPT performed quite well on sentence classification, compared to the other models tested. 
We additionally investigated how different prompting techniques performed. The different prompting techniques produced different results across the different models. For example, for Llama 1B, few-shot performed the best, then 0-shot, then COT. However, for the Llama 3B, 0-shot was the best, then COT, then few-shot. Finally, for the Llama 8B, 0-shot was the best, and few-shot and COT were nearly tied. Overall, we observe that a single prompting technique does not outperform the others reliably. 
</p>
<p>
We also plotted the answers that were one-off - either one too high or one too low - which are graphed in orange. Two-off, three-off, and four-off classifications are graphed as well. The fact that the orange bars are above the blue bars for all but one model (Llama 1B 1-shot) shows that the classification accuracy of the models is generally not the best. However, the orange and blue bars are much higher than the other bars for the smaller models, showing that the models are at least close in their classifications of sentence levels. For the larger models, this isn't necessarily the case. For Llama 8, the percentage of two-off guesses was almost as high as the percentage of correct guesses, especially on 1-shot and COT. This is true for Llama 3 COT as well, and for Llama 3 1-shot the green bar is higher than the blue bar. The red and purple bars are generally low, with the highest being the Llama 3B 1-shot, which is the worst-performing model and prompt combination with a low blue bar, and high green and red bars compared to the other models. 
</p>
<p>
One prominent failure case for one level off is the misclassification of a C1 sentence as B2, for example, "The standard shape for pyrohy is oblong with tapering ends, but rectangular or circular pyrohy are also common." This is due to the misclassification of the vocabulary "pyrohy" as a B2 rather than a C1 word. We also observe failure cases for two level off as all models misclassify a B1 sentence as A1 "My brother is my favorite sibling." Three-off failure examples include misclassifying a C2 sentence as B1 "With approximately 25 phonemes, the German consonant system has an average number of consonants in comparison with other languages." These failure cases might stem from the nuance nature of CEFR language in terms of coherence, fluency and context. This may be resolved by adding more instructions to our prompts to focus more on these topics. We also observe some hallucination by rating an A2 sentence as C2 by Llama 3.2-1B. 
</p>
<p>
ChatGPT also overperforms Llama 3.2-1B in correctly predicting C1 sentences, which are underrated by Llama as B1-B2, for example, "Despite their failure to score any hits, the American torpedo attacks achieved three important results." The main difference in reasoning between these two models is that ChatGPT could capture the complex sentence structure ("despite") and abstract vocabulary ("failure, achieved") that Llama3.2-1B fails to mention. There were also failure cases of Llama 3.2-3B and Llama 3.1-8B underrating sentences while Llama 3.2-1B correctly classifies it; for example, "Like most rattlesnakes, they live on land, and are not good climbers ." is B2 but rated as A2-B1. Surprisingly, the reasoning for these models are very similar to each other.
</p>
<p>
Overall, we were able to successfully show that the size of the model does matter in sentence classification accuracy and that the prompting technique does not appear to predict accuracy across models. 
</p>

<div class="image-container">
  <img src="https://raw.githubusercontent.com/RianaH/MosaicPineapple/main/figures/Fig2Average Difference to Ground Truth label.png" alt="On average, how far off each model was in its guessing from the true label.">
  <p class="image-name">On average, how far off each model was in its guessing from the true label.</p>
</div>
<p>
Figure 2 shows how far off each model was on average and provides another way, in addition to Figure 1 to analyze the best and worst performing models. Again, ChatGPT and the Llama 1B 0-shot and 1-shot perform the best. However, this graph allows us to see that Llama 8B 0-shot performs the fourth best by this metric, instead of the Llama 1B 1-shot. This graph also suggests that the LLama 8Bs performed better than the Llama 3Bs on average. This suggests that it is not a general trend that smaller models perform better but is instead that LLama 1B performs better than larger models. The fact that Llama 3B performs the worst warrants more research into why this is the case. It would be interesting to expand the test to other sizes of models to see what pattern emerges.
</p>
<p>
While there was not a strong prompting-based pattern in the first graph, this graph suggests that 0-shot performs the best, as it is the smallest bar for each of the Llama models. This is surprising, as one might guess that the more you guide a model, the better it performs. However, this suggests that the models perform best without guidance/examples. 
</p>
<p>
Reconciling the results of the first and second graphs, depends on what one cares about most. The first graph is more helpful if you care primarily about the number of true correct guesses and want to base performance on that. Or if you want to avoid the worst performance - e.g. maybe you can avoid any model with a high bar for 4 off. The second graph is more helpful if you care about overall performance and want a measure of which models tend to be more or less off on across all answers. Taking both graphs into consideration together is probably the best approach to identifying the best-performing models.
</p>
<p>
This figure, similar to the first figure, was a success in showing that models perform differently on language-level classification tasks and that the size of the model does matter in classification accuracy. This figure also shows us that it turns out the prompting technique does matter as well, which we were not able to glean from the first figure.
</p>
<p>
Figure 3 explores the research question of whether there is a bias towards predicting too high or too low when incorrect classifications are made. This question gets at whether models have a propensity to think sentences are overly proficient or under-performing. We graph the percent of correct answers alongside the perfect of times each model classified sentences too high in level versus too low in level. Overall, there was a tendency for models to guess too low, meaning they unclassified the proficiency of sentences; every green bar is above its corresponding orange bar. This bias was less present in the better performing models - with it being a near tie for ChatGPT and Llama 1B 0-shot - and most pronounced in the worst performing models (namely Llama 3B 1-shot). The higher the relative size of the green bar to the orange bar, the higher the model's bias towards guessing low. 
</p>
<p>
Overall, we successfully showed that there is a bias for models to under-predict sentence level and that this bias is strongest in the worst-performing models. This suggests that much of the issue of model performance overall is being driven by an under-performance bias. Hence, one way to improve LLM's ability to correctly assess language level may be to introduce methods to promote higher classification. This could involve things like training on a higher subset of more sophisticated sentences or introducing a small-weight term penalizing under-classification. It's possible prompting techniques could also help address this - perhaps including in the prompt a suggested check such as "Please double check that you did not classify the sentence too low". As we learned in class, simply asking models to think step by step or check their answers often results in more accurate answers, so prompting the model to make sure it's not under-guessing could be a useful strategy to test. 
</p>
<p>
Some failure cases of guessing too low or too high include classifying C1-C2 sentences as B1-B2 (for example, "Modern didgeridoo designs are distinct from the traditional Australian Aboriginal didgeridoo, and are innovations recognized by musicologists.", or vice-versa (for example, "Whitbourne has many of the facilities of a small town that has traditionally been a regional service center.").
</p>
<p>
Figure 4 addresses the research question of whether there is a within-letter bias in model prediction when an incorrect guess is one-off. Three within-level incorrect choices can be made when a guess is one-off (A1 vs A2, B1 vs B2, and C1 vs C2). Two between-level choices can be made when a guess is one-off (A2 vs B1, B2 vs C1). We normalized the results to account for the fact that within-level is inherently more likely due to this fact. After normalization, our result shows that there is not a strong bias towards either within or between letter guessing (all fo the results are around chance - 50\%). Nor is there a strong differentiation in bias between model sizes or from different prompting techniques. While there is no general strong bias, there is a slight bias towards guessing between letters for most models, with several of the bars being around 40\%. 
</p>
<p>
This means that many of the models were slightly more likely to bridge a letter than guess within that letter. This was counter to our initial prediction. We thought that there was probably a reason why some levels were classified under the same letter (i.e. A1 and A2 - and why the classification levels weren't just A, B, C, D, E, F instead of A1, A2, B1, B2, C1, C2); The CEFR website states that the levels are grouped as such: A1-A2 (Basic User), B1-B2 (Independent User), and C1-C2 (Proficient User). Hence, we thought that this reason / this grouping may influence the models to be more likely to make mistakes within letter levels than between letter levels. However, this hypothesis was disproved by our results. Despite this, we did successfully test the hypothesis.
</p>
<p>
Figure 5 and Figure 6 explore guessing patterns of the models across the levels A1-C2 compared to the true distribution of the sentence levels. Figure 5 looks at the distribution across total guesses (correct and incorrect) while Figure 6 looks at the distribution across correct guesses only. In both graphs, the true distribution is graphed as a dotted black line. As can be seen from the dotted line, the distribution of sentence levels is not uniform - there is a higher prevalence of intermediate-level sentences (ie B1, B2) than extreme-level sentences (ie A1, C2). This likely is because a majority of regular language is at the intermediate level and it is harder to find or generate super complex or super incoherent sentences. A couple of things are of note from looking at Figures 5 and 6: 1) generally, the model guessing follows the trend of the true distribution. However, there is apparent bias in some models - for example, the red bar (model Llama 1B COT) has a much stronger preference for guessing B1 than is validated by the data's underlying distribution. It also has a stronger preference for A1 than is valid. And a huge bias against A2 and B2. Other models - such as the brown Llama 3B 1-shot show a bias towards A1 and A2 guessing, with a deficit in the others. So, while there is not a general bias trend across all of the models, individual models display distribution biases. Some examples are:
Llama 1B 0-shot (orange): decent bias against B1 and towards B2.
Llama 1B COT (red): large bias for B1 and A1 and against B2 and A2.
Llama 3B 1-shot (brown): large bias for A1 and A2. Deficient for B1, B2, and C1.
Llama 8b COT (teal): bias for A1 and B1 bias against A2 and huge bias against B2. 
While we didn't get to investigate it in this project, it would be interesting to test the models on a CEFR dataset that was equally distributed between levels to see if that affected model performance. 2) When comparing the figures, we can make interesting observations about the relation between total guesses and correct guesses given the underlying distribution. Notably, the correct guesses for A1 are very low and the total guesses for A1 are above the true distribution. This makes some sense, as since there are not very many true A1 sentences, there are not many to guess right / fewer chances to get a correct A1 than with many of the other levels. It also makes sense that if the models expect a more similar underlying distribution why they might be guessing A1 in higher quantities than it truly exists. However, the fact that the A1 correct guesses are far below the true distribution of A1s also suggests that even for the A1s present, the model struggles to get them right. It would be interesting to investigate if this problem was partially alleviated if the model had more A1 sentences to train on. We can also notice that some bars in the correct guessing graph mirror their respective bars in the overall guessing graph while some do not. An example of some bars that do are the red and blue B1 bars and the orange, green, and blue B2 bars. This means that while those models are over-guessing their categories, they are also guessing a large percentage of those guesses correctly. Other bars do not follow this pattern. For example, the brown A2 bar is high in the overall guessing graph and low in the correct guessing graph, meaning that the model is over-guessing A2 to its detriment as it is not often getting those guesses correct. Comparing the two graphs can help identify when an over-guessing strategy might be serving a model decently well vs when it is working highly to the model's detriment. 
</p>
<p>
Overall, these figures are a useful tool to investigate 1) how the models guess compared to the underlying distribution, 2) how much over and under-guessing patterns are hindering model correctness, and 3) which models have biases towards certain sentence levels. They also shed insight into why some models perform better than others and why incorrect guesses occur. We successfully show that while on a whole, the models roughly guess along the underlying data trend, some individual models have strong biases favoring or de-favoring certain classifications, which impacts the overall performance of the models.
</p>
<p>
In the cases that Llama 3.2-1B using CoT prompting guesses B1 wrongly, the correct labels are at most 2 levels off, with only less than $0.05\%$ 2 levels off out of the total predictions. Some example of one-level off include misevaluating B1 as B2 (for example, "It currently has articles, making it the 18th largest Wikipedia by article count.) or vice-versa (for example, "The series also featured at least one significant cameo role per episode.")
</p>
<p>
Figure 7 was meant to address the research question of whether more subjective sentences (i.e. those where the two CEFR annotators disagreed on the sentence level) were harder for the model to classify. Overall, the difference between the red and blue bars are very minimal, showing that the models performance is not predicted by annotator disagreement / sentence subjectivity. Some of the worse performing models (Llama 1B COT, Llama 3Bs and Llama 8B 1-shot and COT) do seem to perform slightly worse on sentences where the annotator disagreed, which means they might be struggling a bit with the subjectivity, but not to a degree that it's a major point to note or worth trying to optimize on in order to increase model performance. Our figure successfully answer our question as to whether sentence subjectivity matters with a "no", especially for the better performing models.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div>
<br><br>

<hr>



<h2 id="conclusion">Conclusion and Future Work</h2>
    
<h3 id="replicability">Replicability</h2>
<p>Our results are extremely replicable. Any dataset of CEFR labeled English text combined with any prompt could be relatively easily run through ChatGPT and Llama models to get similarly formatted results to analyze. Of course the results would likely be different for different datasets, models, and prompts, but that would likely be due more to the annotators used for the dataset and the prompts used, and less to the models themselves. This is once again because the CEFR scale is open to human interpretation.</p>

<h3 id="dataset">Dataset</h2>
<p>Our dataset was large enough to be significant, having about 10000 labeled sentences. The main drawback of our dataset was the use of only two human annotators, who often disagreed. A more fair dataset would have more annotators so a more definitive score could be determined for each sentence. This could have led to many different results, since often the models agreed with the annotator who labeled a sentence at a lower CEFR level, and perhaps with more annotators the true label would have trended down towards that lower level, in which case the model would have had a higher accuracy. Another possible drawback of our dataset is a bias towards higher level labels for longer sentences, which could be a cause for our result that the smaller Llama1B model was the most effective classifier. Our dataset choice did not seem to have an affect on any other people's choice in project to undertake, as our project was quite unique in its goal of testing already-existing language models for this task.</p>

<h3 id="ethics">Ethics</h2>
<p>The only possible risk this work could pose to society is based on the perceived morality of using machine learning for a task as "human" and language proficiency assessment. While we have mentioned that human annotators can be inconsistent at classifying text on the CEFR scale, and that language models could potentially solve this inconsistency problem, there is an argument to be made that only a human should be classifying human-written text. This mainly stems from the fact that most applications of this task are used in very high-importance fields like education, immigration, and work authorizations, all of which can fundamentally change a person's life. For this reason, it is understandable to argue that even a "mostly accurate" model should never be used for this task, since the highest possible accuracy is needed to ensure people's futures are not put at stake due to a false classification. The only real ways to combat this issue are to either develop models that exceedingly accurate in its assessments, or to have a human annotator evaluate the predictions of the model, which defeats the main purpose of using language models for this task in the first place.</p>

<h3 id="futurework">Future Work</h2>
<p>As the results show, our smallest model, Llama1B, was most effective at both correctly classifying and being within 1 CEFR level of the true label of the sentences, while ChatGPT was the second most effective, and Llama3B and Llama8B were both very ineffective. Additionally, ChatGPT and Llama1B were found to both retain accuracy when focusing on the more subjective sentences for which the human annotators of our disagreed, while Llama3B and Llama8B's accuracies went down significantly in this case. This is quite surprising, and more research could be done into the reasons for this behavior if this behavior continues for other models with differing sizes.</p>

<p>Overall, our best exact accuracy was found to be about 45%, and our best within-1-level accuracy was about 90%. These values are lower than previous research into specialized models and methods for language proficiency assessment, which achieved exact accuracies as high as 82% on the CEFR scale. This is to be expected, since the models were not fine-tuned or trained, and instead were used as-is. In the future, more research could be done on the effectiveness of fine-tuned or pre-trained open-source models for this task, to see if they can surpass custom models made for this task.</p>

<p>Another key result is that the models with lower accuracies, specifically Llama3B and Llama8B, were consistently classifying sentences lower than they actually were on the CEFR scale, whereas higher performing models were more evenly split between high and low classifications. A better version of this experiment could have attempted to correct for this by shifting the predicted scores from Llama3B and Llama8B up by a set amount to account for this bias towards lower CEFR scores.</p>

<p>Other ideas for future research include assessing the models' abilities in discriminating between closer and further apart CEFR level sentences, testing the models' efficacy for discriminating between broader and more specific scales (i.e. CEFR with just A, B, and C vs CEFR with A1, A1.5, A2, ...), and more research into which prompting techniques are most effective for this task, since the differing prompting techniques used in our experiment had no clear effects, with each of them being better in some cases and worse in others.</p>


<hr>


  </div>
  


</body></html>
