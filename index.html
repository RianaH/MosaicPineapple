<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">English Proficiency Detection and Generation</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Team Mosaic Pineapple</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="">
            
            
          </div>
          <p>
                        
              Nhi Dang
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="">
            
          </div>
          <p>
            
            Riana Hoagland
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="">            
            
          </div>
          <p>
              Harrison Wallander
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Assessing language proficiency based on the CEFR scale is crucial for educational and professional settings. While SOTA language models like ChatGPT can typically distinguish distant proficiency levels, 
  they struggle with closer distinctions (e.g., A1 vs. A2). This project aims to analyze and compare the performance of multiple models, including ChatGPT, Gemini, and various Llama models (Llama 3.2 1B, Llama 3.2 3B, Llama 3.1 8B), 
  in classifying both distant and adjacent CEFR levels. Our analysis evaluates classification through direct accuracy, ±1 level accuracy, and the best vs. worst accuracy gaps between levels, incorporating advanced prompting techniques, 
  such as few-shot and chain-of-thought prompting. Optional steps include model fine-tuning and using intermediate “between levels” (e.g., A1.5) to refine results. 
  We aim to analyze errors and model performance variations, especially for fine-grained and coarse-grained levels.</p>

<hr>

<h2 id="teaser">Overview </h2>

<img src="https://raw.githubusercontent.com/RianaH/MosaicPineapple/main/one.jpg" alt="Process Overview" width="600" height="400">

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
This study aims to advance language proficiency assessment by evaluating and comparing the performance of several language models—ChatGPT, Gemini, and various Llama models—on the fine-grained classification of CEFR proficiency levels. 
We focus on models' direct accuracy in distinguishing specific levels, as well as their accuracy within one level and across best and worst accuracy gaps between adjacent levels. Additionally, we assess models' capabilities both between and within each proficiency level, 
  contributing to the scalability of automated assessment across the proficiency continuum.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
Current models often struggle to classify language proficiency at a granular level, typically focusing on broader level distinctions or general proficiency scoring. 
  Fine-grained distinctions within and between levels are frequently overlooked, and most efforts depend on model size or basic prompting techniques. 
  These limitations hinder the adaptability and scalability of such models for applications requiring nuanced assessment.
<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
This research adds to the scaling and continuum aspects of model assessment in language proficiency assessment tasks, enhancing the potential for accurate, automated, and scalable language proficiency classification across and within language levels. 
  Success here would enable more precise proficiency evaluations and benefit NLP applications in language education, assessment, and personalized content generation.

</p>

<hr>

<h2 id="problemform_ideas">Problem Formulation/Proposed Ideas</h2>

<p>
<b>Problem Formulation</b>
</p>

<p>
The primary challenge is to accurately assess language proficiency at a fine-grained level across the CEFR proficiency scale (A1–C2), which requires distinguishing both between and within levels. 
  Existing models are effective at broader classification but lack the precision needed for adjacent level distinctions, and their performance is largely model-size dependent. 
  This project investigates two main gaps: 1) models’ ability to classify closely spaced proficiency levels accurately, and 2) how various factors—model type, prompt design, and text features—affect performance on both a coarse-grained (e.g., A vs. B) and fine-grained (e.g., A1 vs. A1.5) scale.
</p>

<p>
<b>Proposed Ideas</b>
</p>

<p>To address these challenges, we propose:</p>

<ol>
    <li><strong>Model comparisons (including fine-grained models with varying sizes)</strong>
        <ul>
            <li>Evaluate direct accuracy, ±1 level accuracy, and gaps in accuracy between best and worst adjacent-level distinctions.</li>
            <li>This will include models of various sizes, specifically Llama models (Llama 3.2 1B, Llama 3.2 3B, Llama 3.1 8B), ChatGPT, and Gemini.</li>
            <li>We will investigate if higher model size and complexity correspond to improved accuracy at both coarse-grained and fine-grained levels.</li>
        </ul>
    </li>
  
    <li><strong>Handling between-level discrepancies:</strong>
        <ul>
            <li>For cases where classifications fall between levels (e.g., between A1 and A2), use an interpolated level system (e.g., A1.5).</li>
            <li>This approach allows us to capture finer gradients in model responses and to compare performance across standard CEFR levels and interpolated, in-between levels.</li>
        </ul>
    </li>  

    <li><strong>Exploring different prompting techniques:</strong>
        <ul>
            <li>Apply prompting strategies such as few-shot learning with retrieval-augmented generation (RAG) and chain-of-thought reasoning.</li>
            <li>Assess how prompt design impacts model performance on fine-grained distinctions.</li>
        </ul>
    </li>

    <li><strong>Fine-Tuning (Optional):</strong>
        <ul>
            <li>If time allows, experiment with fine-tuning select models on CEFR-labeled datasets to explore potential improvements in differentiating closely spaced levels.</li>
            <li>Fine-tuning would not be a primary focus but could help assess the extent to which tailored training enhances proficiency classification.</li>
        </ul>
    </li>
</ol>

<!-- PLEASE UNCOMMENT THIS IF NEEDED [STARTING HERE]
<hr>    
    
<h2 id="problemform_ideas">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.
</p> [ENDING HERE] -->

<hr>    

<h2 id="approach">Preliminary result</h2>

<p>ChatGPT-3 exhibited wrong estimation, such as estimating a question to be more difficult for C1-level learners (i.e., “Proficient”) than A1-level learners (i.e., “Basic”) <a href="https://arxiv.org/pdf/2307.08393">[1]</a>.</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
    </tr>
  </tbody>
  <caption>Table 1. Comparison of A1-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">C1</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">A2</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
    </tr>
  </tbody>
  <caption>Table 2. Comparison of A2-level classification across Llama models and prompting techniques.</caption>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Prompting Technique</strong></th>
      <th style="text-align: center"><strong>True Level</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 1B</strong></th>
      <th style="text-align: center"><strong>Llama 3.2 3B</strong></th>
      <th style="text-align: center"><strong>Llama 3.1 8B</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>No-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Few-shot Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">B2</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>System/Instructional Prompting</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">A1</td>
      <td style="text-align: center">C1</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chain-of-thought</strong></td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B1</td>
      <td style="text-align: center">B2</td>
      <td style="text-align: center">A1</td>
    </tr>
  </tbody>
  <caption>Table 3. Comparison of B1-level classifications across Llama models and prompting techniques.</caption>
</table>

<h4>Error analysis of model performance in CEFR level classification</h4>

<ul>
    <li><strong>Llama 3.2 1B</strong> achieved an overall accuracy of <strong>75%</strong> for B1 sentence. This model frequently overrated A1-level sentences as A2 or B2. However, it correctly predicted B1-level sentence, despite the small size of the model.</li>

    <li><strong>Llama 3.2 3B</strong> demonstrated some variability in its performance. This model often rated A1-level sentences as A2 or B1. Additionally, it frequently classified A2-level sentences as B1, B2, or even C1. There were instances where B1 sentences were underrated and classified as A1 when using system prompting. This model highlighted critical inconsistencies in its classifications.</li>

    <li><strong>Llama 3.1 8B</strong> displayed the most diverse classification outcomes, which resulted in the greatest variability in performance. This model consistently rated A1-level sentences as A2 or B2 and often classified A2-level sentences as B1 or B2. Furthermore, it varied in its classification of B1 sentences, with some being downgraded to A1 or A2 while others were classified as C1 or B2. This model showcased a lack of stability in its predictions, even though it has the biggest size across 3 models.</li>
</ul>
<hr>

    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">1</th>
      <th style="text-align: center">2</th>
      <th style="text-align: center">3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Sentence</strong></td>
      <td style="text-align: center">Example 1</td>
      <td style="text-align: center">Example 2</td>
      <td style="text-align: center">Example 3</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Errors</strong></td>
      <td style="text-align: center">error A, error B, error C</td>
      <td style="text-align: center">error C</td>
      <td style="text-align: center">error B</td>
    </tr>
  </tbody>
  <caption>Table 1. This is Table 1's caption</caption>
</table>
<br>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="./files/results.png">
</div>
<br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>


<hr>


  </div>
  


</body></html>
